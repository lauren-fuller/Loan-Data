---
title: "Loan Data"
output: html_document
date: "2025-04-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(fastDummies)
library(smotefamily)
library(caret)     # For model training and evaluation
library(ROCR) 
library(glmnet)

```

## Load the Data

```{r cars}
file_path <- "/Users/laurenfuller/Desktop/LoanStats3a.csv"

# Define the column names you want to keep
selected_columns <- c("loan_status", "loan_amnt", "term", "addr_state","int_rate", "grade", "sub_grade", "emp_length",
                      "home_ownership", "annual_inc", "verification_status",
                      "pymnt_plan", "purpose", "dti", "delinq_2yrs",
                      "inq_last_6mths", "pub_rec", "revol_util", "total_acc",
                      "pub_rec_bankruptcies")

# Read the CSV and keep only the selected columns
df <- read_csv(file_path, col_select = all_of(selected_columns))
df$int_rate <- substr(df$int_rate, 1, nchar(df$int_rate) - 1)
df$int_rate <- as.numeric(df$int_rate) * .01

df$revol_util <- substr(df$revol_util, 1, nchar(df$revol_util) - 1)
df$revol_util <- as.numeric(df$revol_util) * .01

#df
```

## How many unique values do we have in the categorical columns? We're going to get rid of the variables that have too many unique values.

```{r unique values, echo=FALSE}
# Loop through each column and print unique values
# List of target columns
cols <- c("loan_status","term", "addr_state", "grade", "sub_grade", "emp_length", "home_ownership",
          "verification_status", "pymnt_plan", "purpose")

df$loan_status <- case_when(
  str_detect(df$loan_status, "Charged Off") ~ "Charged Off",
  str_detect(df$loan_status, "Fully Paid") ~ "Fully Paid",
  str_detect(df$loan_status, "Default") ~ "Default",
  str_detect(df$loan_status, "Current") ~ "Current",
  str_detect(df$loan_status, "In Grace Period") ~ "In Grace Period",
  str_detect(df$loan_status, "Late") ~ "Late"
  #TRUE ~ NA_character_ # Set all other statuses to NA
)

# Loop through and show value counts
for (col in cols) {
  cat("\nValue counts for", col, ":\n")
  print(table(df[[col]]))
}
```

## Drop sub_grade, get rid of rows that have N/A and blanks, consolidate categories in variables

```{r categorical cleaning, echo=FALSE}
#drop subgrade - too many variables
df <- df %>% select(-sub_grade)

#drop pymnt_plan - only 1 yes, the rest are no
df <- df %>% select(-pymnt_plan)

#remove rows with blanks & N/A
  # First, replace blank strings with NA
df[df == ""] <- NA

  # Then, remove any rows with NA
df <- na.omit(df)

# Group emp_length into 4 sections
df <- df %>% filter(emp_length != "n/a")
df <- df %>%
  mutate(emp_length = str_trim(emp_length)) %>%
  mutate(emp_length = case_when(
    emp_length %in% c("< 1 year", "1 year", "2 years", "3 years") ~ "0-3 years",
    emp_length %in% c("4 years", "5 years", "6 years") ~ "4-6 years",
    emp_length %in% c("7 years", "8 years", "9 years") ~ "7-9 years",
    emp_length == "10+ years" ~ "10+ years",
    TRUE ~ NA_character_
  ))

#consolidate state into 4 regions
df <- df %>%
  mutate(region = case_when(
    addr_state %in% c("CT", "ME", "MA", "NH", "RI", "VT", "NJ", "NY", "PA") ~ "Northeast",
    addr_state %in% c("IL", "IN", "MI", "OH", "WI", "IA", "KS", "MN", "MO", "NE", "SD") ~ "Midwest",
    addr_state %in% c("DE", "DC", "FL", "GA", "MD", "NC", "SC", "VA", "WV",
                      "AL", "KY", "MS", "TN", "AR", "LA", "OK", "TX") ~ "South",
    addr_state %in% c("AK", "AZ", "CA", "CO", "HI", "ID", "MT", "NV", "NM",
                      "OR", "UT", "WA", "WY") ~ "West",
    TRUE ~ NA_character_
  ))
df <- df %>% select(-addr_state)



#remove rows where home_ownership is "NONE" or "OTHER" - there are only like 150/45000
df <- df %>% filter(!(home_ownership %in% c("NONE", "OTHER")))

#consolidate the purpose section into fewer categories
df <- df %>%
  mutate(purpose_group = case_when(
    purpose %in% c("debt_consolidation", "credit_card") ~ "Debt",
    purpose %in% c("home_improvement", "house", "moving") ~ "Home",
    purpose %in% c("major_purchase", "car", "wedding", "vacation") ~ "Big Purchase",
    purpose %in% c("medical", "other", "renewable_energy", "small_business", "educational") ~ "Medical/Other",
    TRUE ~ NA_character_
  ))

#drop purpose - too many variables
df <- df %>% select(-purpose)


cols <- c("loan_status", "term", "region", "grade", "emp_length", "home_ownership",
          "verification_status", "purpose_group")

# Loop through and show value counts
for (col in cols) {
  cat("\nValue counts for", col, ":\n")
  print(table(df[[col]]))
}
```

``` {r dummy variables}
df <- df %>%
  fastDummies::dummy_cols(
    select_columns = c("term", "region", "grade", "emp_length",
                       "home_ownership", "verification_status", "purpose_group"),
    remove_first_dummy = FALSE,       # keep all dummy variables
    remove_selected_columns = TRUE    # drop the original columns
  )

```


## Create two dataframes and finalize df_loans to make model

``` {r}
# First group: loans with clear outcomes
df_loans <- df %>%
  filter(loan_status %in% c("Charged Off", "Default", "Fully Paid"))

# Second group: loans still in progress or at risk
df_predict <- df %>%
  filter(loan_status %in% c("Current", "In Grace Period", "Late"))

df_loans <- df %>%
  filter(loan_status %in% c("Charged Off", "Default", "Fully Paid")) %>%
  mutate(unpaid_loan = if_else(loan_status %in% c("Charged Off", "Default"), 1, 0))

df_loans <- df_loans %>% select(-loan_status)


write.csv(df_loans, "/Users/laurenfuller/Desktop/df_loans.csv", row.names = FALSE)
write.csv(df_predict, "/Users/laurenfuller/Desktop/df_predict.csv", row.names = FALSE)

```

# Logistc Regression

## Handling Class Imbalances

``` {r}
# Bar plot of uneven class distribution bewtween unpaid and paid loans.
# This is a problem because we want to be able to predict an unpaid loan, and there are far less of them then paid loans
counts <- table(df_loans$unpaid_loan)
colors <- c("lightblue", "salmon")

par(mar = c(5, 4, 4, 8), xpd = TRUE)

bp <- barplot(counts,
        main = "Class Balance of Response Variable",
        xlab = "Unpaid Loan Status",
        ylab = "Count",
        col = colors,
        names.arg = c("Paid - 0", "Unpaid - 1"))

text(x = bp, 
     y = counts, 
     label = counts, 
     pos = 3,  # position 3 = above
     cex = 0.8, 
     col = "black")

legend("topright",
       inset = c(-0.15, 0),
       legend = c("Paid - 0", "Unpaid - 1"),
       fill = colors,
       title = "Loan Status",
       bty = "n")
``` 
## Balance the Data set using SMOTE

``` {r}
library(smotefamily)

# Ensure the target variable is a factor for classification
df_loans$unpaid_loan <- as.factor(df_loans$unpaid_loan)

# Create the dataset X (features without target variable) and target (target variable)
X <- df_loans[, -which(names(df_loans) == "unpaid_loan")]
target <- df_loans$unpaid_loan

# Balance the dataset using SMOTE (oversample minority class)
balanced_df <- SMOTE(X = X, 
                     target = target, 
                     K = 5, 
                     dup_size = 4)  # Duplication factor for synthetic instances

# Check the structure of the balanced_df object
#str(balanced_df)

# Check if the target variable is included in the balanced dataset and inspect column names
#names(balanced_df$data)

table(balanced_df$data$class)
```
``` {r}
# Bar plot of uneven class distribution bewtween unpaid and paid loans.
# This is a problem because we want to be able to predict an unpaid loan, and there are far less of them then paid loans
counts <- table(balanced_df$data$class)
colors <- c("lightblue", "salmon")

par(mar = c(5, 4, 4, 8), xpd = TRUE)

bp <- barplot(counts,
        main = "Class Balance of Response Variable - After SMOTE",
        xlab = "Unpaid Loan Status",
        ylab = "Count",
        col = colors,
        names.arg = c("Paid - 0", "Unpaid - 1"))

text(x = bp, 
     y = counts, 
     label = counts, 
     pos = 3,  # position 3 = above
     cex = 0.8, 
     col = "black")

legend("topright",
       inset = c(-0.25, 0),
       legend = c("Paid - 0", "Unpaid - 1"),
       fill = colors,
       title = "Loan Status",
       bty = "n")
``` 


``` {r}
#standardize the data
balanced_df$data <- balanced_df$data %>%
  mutate(across(c(loan_amnt, int_rate, annual_inc, dti, delinq_2yrs, 
                  inq_last_6mths, pub_rec, revol_util, total_acc, pub_rec_bankruptcies),
                scale))

# Ensure unpaid_loan is numeric (0 = paid, 1 = unpaid)
colnames(balanced_df$data)[colnames(balanced_df$data) == "class"] <- "unpaid_loan"
balanced_df$data$unpaid_loan <- as.numeric(balanced_df$data$unpaid_loan)

# 1. Split into train/test
set.seed(123)  # for reproducibility
sample_index <- sample(1:nrow(balanced_df$data), 0.7 * nrow(balanced_df$data))  # 70% training
train_data <- balanced_df$data[sample_index, ]
test_data <- balanced_df$data[-sample_index, ]

# 2. Fit logistic regression on train
log_base_model <- glm(unpaid_loan ~ ., data = train_data, family = "binomial")

summary(log_base_model)

# 3. Predict on test
test_data$predicted_prob <- predict(log_base_model, newdata = test_data, type = "response")
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, 1, 0)

# 4. Evaluate
table(Predicted = test_data$predicted_class, Actual = test_data$unpaid_loan)
confusionMatrix(as.factor(test_data$predicted_class), as.factor(test_data$unpaid_loan))

```

# Logistic LASSO - glmnet apparently already does standardization
``` {r}
# Make sure your training features are a matrix
x_train <- as.matrix(train_data %>% select(-unpaid_loan))
y_train <- train_data$unpaid_loan

x_test <- as.matrix(test_data %>% select(-unpaid_loan, -predicted_prob, -predicted_class))
y_test <- test_data$unpaid_loan


# Fit LASSO Logistic Regression
set.seed(123)
lasso_model <- glmnet(x_train, y_train, 
                      family = "binomial", 
                      alpha = 1)  # alpha = 1 means LASSO

# Plot the coefficient paths
plot(lasso_model, xvar = "lambda")

# Cross-validation to find the best lambda
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, 
                      family = "binomial", 
                      alpha = 1, 
                      type.measure = "class")

# Plot CV curve
plot(cv_lasso)

# Best lambda
best_lambda <- cv_lasso$lambda.min
best_lambda

# Fit final model at best lambda
best_lasso_model <- glmnet(x_train, y_train, 
                            family = "binomial", 
                            alpha = 1, 
                            lambda = best_lambda)

# Coefficients
coef(best_lasso_model)

# Predict on test data
predicted_prob <- predict(best_lasso_model, s = best_lambda, newx = x_test, type = "response")
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)

# Evaluate
table(Predicted = predicted_class, Actual = y_test)
confusionMatrix(as.factor(predicted_class), as.factor(y_test))

```


# Compare models
``` {r}
# Compare base logistic and lasso logistic model performance

# 1. Confusion matrix for base logistic regression
conf_matrix_base <- confusionMatrix(as.factor(test_data$predicted_class), as.factor(test_data$unpaid_loan))

# 2. Confusion matrix for LASSO model
conf_matrix_lasso <- confusionMatrix(as.factor(predicted_class), as.factor(y_test))

# 3. Extract relevant metrics
metrics_comparison <- tibble(
  Metric = c("Accuracy", "Sensitivity (Recall)", "Specificity"),
  Logistic_Regression = c(
    conf_matrix_base$overall["Accuracy"],
    conf_matrix_base$byClass["Sensitivity"],
    conf_matrix_base$byClass["Specificity"]
  ),
  LASSO_Regression = c(
    conf_matrix_lasso$overall["Accuracy"],
    conf_matrix_lasso$byClass["Sensitivity"],
    conf_matrix_lasso$byClass["Specificity"]
  )
)

# 4. Show side-by-side
print(metrics_comparison)

```



``` {r}
```

``` {r}
```


``` {r}
```


``` {r}
```

``` {r}
```

``` {r}
```


